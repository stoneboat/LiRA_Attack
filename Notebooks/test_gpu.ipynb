{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "933f005c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Environment Check\n",
      "============================================================\n",
      "\n",
      "Python executable: /tmp/python-venv/lra_venv/bin/python\n",
      "Python version: 3.9.25 (main, Nov  3 2025, 22:33:05) \n",
      "[GCC 11.2.0]\n",
      "\n",
      "CUDA_HOME: /usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/cuda-11.7.0-iyx5xnjk3fbe2fqhogxbzfzpj7xhi77t\n",
      "CUDNNROOT: /usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/cudnn-8.5.0.96-11.7-k7jh7qvot2tupopuooyvp2hlkss75nhc\n",
      "\n",
      "LD_LIBRARY_PATH (first 300 chars):\n",
      "  /usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/cudnn-8.5.0.96-11.7-k7jh7qvot2tupopuooyvp2hlkss75nhc/lib:/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/cuda-11.7.0-iyx5xnjk3fbe2fqhogxbzfzpj7xhi77t/lib64:/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64...\n",
      "\n",
      "============================================================\n",
      "Library Check\n",
      "============================================================\n",
      "  ✅ CUDA Runtime: /usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/cuda-11.7.0-iyx5xnjk3fbe2fqhogxbzfzpj7xhi77t/lib64/libcudart.so\n",
      "  ✅ cuDNN: /usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/cudnn-8.5.0.96-11.7-k7jh7qvot2tupopuooyvp2hlkss75nhc/lib/libcudnn.so\n",
      "  ✅ cuBLAS: /usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/cuda-11.7.0-iyx5xnjk3fbe2fqhogxbzfzpj7xhi77t/lib64/libcublas.so\n",
      "  ✅ cuFFT: /usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/cuda-11.7.0-iyx5xnjk3fbe2fqhogxbzfzpj7xhi77t/lib64/libcufft.so\n",
      "\n",
      "============================================================\n",
      "TensorFlow GPU Detection\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 08:23:54.065374: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-05 08:23:54.122327: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-05 08:23:54.985511: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TensorFlow version: 2.13.0\n",
      "\n",
      "All physical devices:\n",
      "  - PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "  - PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "\n",
      "GPU devices: 1\n",
      "✅ GPU is available!\n",
      "  GPU 0: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "    Details: {'compute_capability': (7, 0), 'device_name': 'Tesla V100-PCIE-16GB'}\n",
      "\n",
      "============================================================\n",
      "GPU Test\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 08:23:56.669151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14728 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:3b:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU computation test successful!\n",
      "   Result: [[1. 3.]\n",
      " [3. 7.]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Environment Check\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check environment variables\n",
    "print(f\"\\nPython executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "print(f\"\\nCUDA_HOME: {os.environ.get('CUDA_HOME', 'NOT SET')}\")\n",
    "print(f\"CUDNNROOT: {os.environ.get('CUDNNROOT', 'NOT SET')}\")\n",
    "\n",
    "ld_path = os.environ.get('LD_LIBRARY_PATH', 'NOT SET')\n",
    "print(f\"\\nLD_LIBRARY_PATH (first 300 chars):\")\n",
    "print(f\"  {ld_path[:300]}...\")\n",
    "\n",
    "# Check if CUDA libraries are accessible\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Library Check\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import ctypes.util\n",
    "import glob\n",
    "\n",
    "libs_to_check = {\n",
    "    'libcudart': 'CUDA Runtime',\n",
    "    'libcudnn': 'cuDNN',\n",
    "    'libcublas': 'cuBLAS',\n",
    "    'libcufft': 'cuFFT',\n",
    "}\n",
    "\n",
    "for lib_name, lib_desc in libs_to_check.items():\n",
    "    # Try to find library\n",
    "    found_paths = []\n",
    "    if 'LD_LIBRARY_PATH' in os.environ:\n",
    "        for path in os.environ['LD_LIBRARY_PATH'].split(':'):\n",
    "            if path:\n",
    "                matches = glob.glob(f\"{path}/{lib_name}.so*\")\n",
    "                if matches:\n",
    "                    found_paths.extend(matches)\n",
    "    \n",
    "    if found_paths:\n",
    "        print(f\"  ✅ {lib_desc}: {found_paths[0]}\")\n",
    "    else:\n",
    "        # Try ctypes\n",
    "        lib_path = ctypes.util.find_library(lib_name)\n",
    "        if lib_path:\n",
    "            print(f\"  ✅ {lib_desc}: {lib_path}\")\n",
    "        else:\n",
    "            print(f\"  ❌ {lib_desc}: NOT FOUND\")\n",
    "\n",
    "# Test TensorFlow\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TensorFlow GPU Detection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"\\nTensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# List all physical devices\n",
    "print(\"\\nAll physical devices:\")\n",
    "all_devices = tf.config.list_physical_devices()\n",
    "for device in all_devices:\n",
    "    print(f\"  - {device}\")\n",
    "\n",
    "# Check GPU devices specifically\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"\\nGPU devices: {len(gpus)}\")\n",
    "if len(gpus) > 0:\n",
    "    print(\"✅ GPU is available!\")\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"  GPU {i}: {gpu}\")\n",
    "        # Try to get GPU details\n",
    "        try:\n",
    "            gpu_details = tf.config.experimental.get_device_details(gpu)\n",
    "            print(f\"    Details: {gpu_details}\")\n",
    "        except:\n",
    "            pass\n",
    "else:\n",
    "    print(\"❌ No GPU devices detected\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"  1. Check if wrapper script is being used (check ~/.jupyter/kernel_wrapper_debug.log)\")\n",
    "    print(\"  2. Verify LD_LIBRARY_PATH includes CUDA libraries\")\n",
    "    print(\"  3. Make sure TensorFlow module is loaded\")\n",
    "    print(\"  4. Try: import tensorflow as tf; print(tf.config.list_physical_devices())\")\n",
    "\n",
    "# Test GPU computation if available\n",
    "if len(gpus) > 0:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"GPU Test\")\n",
    "    print(\"=\" * 60)\n",
    "    try:\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "            b = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n",
    "            c = tf.matmul(a, b)\n",
    "            print(f\"✅ GPU computation test successful!\")\n",
    "            print(f\"   Result: {c.numpy()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ GPU computation test failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lra-env)",
   "language": "python",
   "name": "lra-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
