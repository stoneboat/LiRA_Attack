{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72786dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 20:37:51.000794: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-05 20:37:51.057575: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-05 20:37:51.898683: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from objax.util import EasyDict\n",
    "from absl import flags\n",
    "\n",
    "# Navigate to the parent directory of the project structure\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), '../../'))\n",
    "src_dir = os.path.join(project_dir, 'src')\n",
    "data_dir = os.path.join(project_dir, 'data')\n",
    "fig_dir = os.path.join(project_dir, 'fig')\n",
    "logs_dir = os.path.join(project_dir, 'logs')\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "# Add the src directory to sys.path\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4524fd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/python-venv/lra_venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import objax\n",
    "from train import MemModule, network\n",
    "\n",
    "# ============================================================================\n",
    "# Inference Parameters \n",
    "# ============================================================================\n",
    "dataset = 'cifar10'\n",
    "dataset_size = 50000\n",
    "base_logdir = os.path.join(logs_dir, 'exp', dataset)\n",
    "os.makedirs(base_logdir, exist_ok=True)\n",
    "regex_pattern = '.*experiment.*'\n",
    "\n",
    "# Epoch configuration\n",
    "from_epoch = None  # If None, will use max_epoch-1. Otherwise, start from this epoch\n",
    "\n",
    "# Batch size for processing\n",
    "batch_size = 5000\n",
    "\n",
    "# ============================================================================\n",
    "# Helper Functions\n",
    "# ============================================================================\n",
    "\n",
    "# Disable GPU for TensorFlow (JAX will handle GPU)\n",
    "tf.config.experimental.set_visible_devices([], \"GPU\")\n",
    "\n",
    "def load_model(arch, nclass, mnist=False):\n",
    "    \"\"\"Load a model with the given architecture.\"\"\"\n",
    "    return MemModule(\n",
    "        network(arch), \n",
    "        nclass=nclass,\n",
    "        mnist=mnist,\n",
    "        arch=arch,\n",
    "        lr=0.1,\n",
    "        batch=0,\n",
    "        epochs=0,\n",
    "        weight_decay=0\n",
    "    )\n",
    "\n",
    "def get_loss(model, xbatch, ybatch, shift=0, reflect=True, stride=1):\n",
    "    \"\"\"\n",
    "    Generate logits for a batch with data augmentation.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to use for inference\n",
    "        xbatch: Input batch (N, H, W, C)\n",
    "        ybatch: Labels (not used, but kept for compatibility)\n",
    "        shift: Shift amount for augmentation (default: 0)\n",
    "        reflect: Whether to use reflection augmentation (default: True)\n",
    "        stride: Stride for augmentation (default: 1)\n",
    "    \n",
    "    Returns:\n",
    "        Logits array of shape (N, num_augs, nclass)\n",
    "    \"\"\"\n",
    "    outs = []\n",
    "    for aug in [xbatch, xbatch[:,:,::-1,:]][:reflect+1]:\n",
    "        aug_pad = tf.pad(aug, [[0] * 2, [shift] * 2, [shift] * 2, [0] * 2], mode='REFLECT').numpy()\n",
    "        for dx in range(0, 2*shift+1, stride):\n",
    "            for dy in range(0, 2*shift+1, stride):\n",
    "                this_x = aug_pad[:, dx:dx+32, dy:dy+32, :].transpose((0,3,1,2))\n",
    "                logits = model.model(this_x, training=True)\n",
    "                outs.append(logits)\n",
    "    \n",
    "    print(f\"Generated logits shape: {np.array(outs).shape}\")\n",
    "    return np.array(outs).transpose((1, 0, 2))\n",
    "\n",
    "def features(model, xbatch, ybatch):\n",
    "    \"\"\"Generate features (logits) for a batch.\"\"\"\n",
    "    return get_loss(model, xbatch, ybatch, shift=0, reflect=True, stride=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa9e382c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 50000 training examples\n",
      "   Data shape: (50000, 32, 32, 3)\n",
      "   Labels shape: (50000,)\n",
      "Dataset: cifar10, Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Load Training Data\n",
    "# ============================================================================\n",
    "# Load the training data that was used for training\n",
    "# This should be in the base logdir (where x_train.npy and y_train.npy are stored)\n",
    "data_path = os.path.join(base_logdir, \"x_train.npy\")\n",
    "labels_path = os.path.join(base_logdir, \"y_train.npy\")\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    # Try alternative location (in logs_dir root)\n",
    "    data_path = os.path.join(logs_dir, \"x_train.npy\")\n",
    "    labels_path = os.path.join(logs_dir, \"y_train.npy\")\n",
    "\n",
    "if os.path.exists(data_path) and os.path.exists(labels_path):\n",
    "    xs_all = np.load(data_path)[:dataset_size]\n",
    "    ys_all = np.load(labels_path)[:dataset_size]\n",
    "    print(f\"‚úÖ Loaded {len(xs_all)} training examples\")\n",
    "    print(f\"   Data shape: {xs_all.shape}\")\n",
    "    print(f\"   Labels shape: {ys_all.shape}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find x_train.npy and y_train.npy. \"\n",
    "        f\"Tried: {os.path.join(base_logdir, 'x_train.npy')} and {os.path.join(logs_dir, 'x_train.npy')}\"\n",
    "    )\n",
    "\n",
    "# Determine number of classes\n",
    "assert dataset in ['cifar10', 'cifar100', 'mnist']\n",
    "nclass = 100 if dataset == 'cifar100' else 10\n",
    "mnist = (dataset == 'mnist')\n",
    "\n",
    "print(f\"Dataset: {dataset}, Number of classes: {nclass}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54617076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 directories in /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/lira_attack/logs/exp/cifar10\n",
      "Filtering with regex: .*experiment.*\n",
      "================================================================================\n",
      "Processing experiment: experiment-0_16\n",
      "Architecture: wrn28-2\n",
      "================================================================================\n",
      "Loading model with architecture: wrn28-2\n",
      "Resuming from /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/lira_attack/logs/exp/cifar10/experiment-0_16/ckpt/0000000100.npz\n",
      "  Found checkpoints up to epoch 100\n",
      "  Processing epochs from 99 to 100\n",
      "  ‚è≠Ô∏è  Skipping epoch 100 (logits already exist)\n",
      "\n",
      "================================================================================\n",
      "Processing experiment: experiment-10_16\n",
      "Architecture: wrn28-2\n",
      "================================================================================\n",
      "Resuming from /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/lira_attack/logs/exp/cifar10/experiment-10_16/ckpt/0000000100.npz\n",
      "  Found checkpoints up to epoch 100\n",
      "  Processing epochs from 99 to 100\n",
      "  ‚è≠Ô∏è  Skipping epoch 100 (logits already exist)\n",
      "\n",
      "================================================================================\n",
      "Processing experiment: experiment-11_16\n",
      "Architecture: wrn28-2\n",
      "================================================================================\n",
      "Resuming from /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/lira_attack/logs/exp/cifar10/experiment-11_16/ckpt/0000000100.npz\n",
      "  Found checkpoints up to epoch 100\n",
      "  Processing epochs from 99 to 100\n",
      "  ‚è≠Ô∏è  Skipping epoch 100 (logits already exist)\n",
      "\n",
      "================================================================================\n",
      "Processing experiment: experiment-12_16\n",
      "Architecture: wrn28-2\n",
      "================================================================================\n",
      "Resuming from /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/lira_attack/logs/exp/cifar10/experiment-12_16/ckpt/0000000100.npz\n",
      "  Found checkpoints up to epoch 100\n",
      "  Processing epochs from 99 to 100\n",
      "  ‚è≠Ô∏è  Skipping epoch 100 (logits already exist)\n",
      "\n",
      "================================================================================\n",
      "Processing experiment: experiment-13_16\n",
      "Architecture: wrn28-2\n",
      "================================================================================\n",
      "Resuming from /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/lira_attack/logs/exp/cifar10/experiment-13_16/ckpt/0000000100.npz\n",
      "  Found checkpoints up to epoch 100\n",
      "  Processing epochs from 99 to 100\n",
      "  ‚è≠Ô∏è  Skipping epoch 100 (logits already exist)\n",
      "\n",
      "================================================================================\n",
      "Processing experiment: experiment-14_16\n",
      "Architecture: wrn28-2\n",
      "================================================================================\n",
      "Resuming from /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/lira_attack/logs/exp/cifar10/experiment-14_16/ckpt/0000000100.npz\n",
      "  Found checkpoints up to epoch 100\n",
      "  Processing epochs from 99 to 100\n",
      "  ‚è≠Ô∏è  Skipping epoch 100 (logits already exist)\n",
      "\n",
      "================================================================================\n",
      "Processing experiment: experiment-15_16\n",
      "Architecture: wrn28-2\n",
      "================================================================================\n",
      "Resuming from /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/lira_attack/logs/exp/cifar10/experiment-15_16/ckpt/0000000100.npz\n",
      "  Found checkpoints up to epoch 100\n",
      "  Processing epochs from 99 to 100\n",
      "  ‚è≠Ô∏è  Skipping epoch 100 (logits already exist)\n",
      "\n",
      "================================================================================\n",
      "Processing experiment: experiment-1_16\n",
      "Architecture: wrn28-2\n",
      "================================================================================\n",
      "Resuming from /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/lira_attack/logs/exp/cifar10/experiment-1_16/ckpt/0000000100.npz\n",
      "  Found checkpoints up to epoch 100\n",
      "  Processing epochs from 99 to 100\n",
      "  ‚è≠Ô∏è  Skipping epoch 100 (logits already exist)\n",
      "\n",
      "================================================================================\n",
      "Processing experiment: experiment-2_16\n",
      "Architecture: wrn28-2\n",
      "================================================================================\n",
      "Resuming from /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/lira_attack/logs/exp/cifar10/experiment-2_16/ckpt/0000000100.npz\n",
      "  Found checkpoints up to epoch 100\n",
      "  Processing epochs from 99 to 100\n",
      "  ‚è≠Ô∏è  Skipping epoch 100 (logits already exist)\n",
      "\n",
      "================================================================================\n",
      "Processing experiment: experiment-3_16\n",
      "Architecture: wrn28-2\n",
      "================================================================================\n",
      "Resuming from /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/lira_attack/logs/exp/cifar10/experiment-3_16/ckpt/0000000100.npz\n",
      "  Found checkpoints up to epoch 100\n",
      "  Processing epochs from 99 to 100\n",
      "  ‚è≠Ô∏è  Skipping epoch 100 (logits already exist)\n",
      "\n",
      "================================================================================\n",
      "Processing experiment: experiment-4_16\n",
      "Architecture: wrn28-2\n",
      "================================================================================\n",
      "Resuming from /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/lira_attack/logs/exp/cifar10/experiment-4_16/ckpt/0000000100.npz\n",
      "  Found checkpoints up to epoch 100\n",
      "  Processing epochs from 99 to 100\n",
      "  ‚è≠Ô∏è  Skipping epoch 100 (logits already exist)\n",
      "\n",
      "================================================================================\n",
      "Processing experiment: experiment-5_16\n",
      "Architecture: wrn28-2\n",
      "================================================================================\n",
      "Resuming from /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/lira_attack/logs/exp/cifar10/experiment-5_16/ckpt/0000000100.npz\n",
      "  Found checkpoints up to epoch 100\n",
      "  Processing epochs from 99 to 100\n",
      "  ‚è≠Ô∏è  Skipping epoch 100 (logits already exist)\n",
      "\n",
      "================================================================================\n",
      "Processing experiment: experiment-6_16\n",
      "Architecture: wrn28-2\n",
      "================================================================================\n",
      "Resuming from /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/lira_attack/logs/exp/cifar10/experiment-6_16/ckpt/0000000100.npz\n",
      "  Found checkpoints up to epoch 100\n",
      "  Processing epochs from 99 to 100\n",
      "  ‚è≠Ô∏è  Skipping epoch 100 (logits already exist)\n",
      "\n",
      "================================================================================\n",
      "Processing experiment: experiment-7_16\n",
      "Architecture: wrn28-2\n",
      "================================================================================\n",
      "Resuming from /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/lira_attack/logs/exp/cifar10/experiment-7_16/ckpt/0000000100.npz\n",
      "  Found checkpoints up to epoch 100\n",
      "  Processing epochs from 99 to 100\n",
      "  ‚è≠Ô∏è  Skipping epoch 100 (logits already exist)\n",
      "\n",
      "================================================================================\n",
      "Processing experiment: experiment-8_16\n",
      "Architecture: wrn28-2\n",
      "================================================================================\n",
      "Resuming from /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/lira_attack/logs/exp/cifar10/experiment-8_16/ckpt/0000000100.npz\n",
      "  Found checkpoints up to epoch 100\n",
      "  Processing epochs from 99 to 100\n",
      "  ‚è≠Ô∏è  Skipping epoch 100 (logits already exist)\n",
      "\n",
      "================================================================================\n",
      "Processing experiment: experiment-9_16\n",
      "Architecture: wrn28-2\n",
      "================================================================================\n",
      "Resuming from /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/lira_attack/logs/exp/cifar10/experiment-9_16/ckpt/0000000100.npz\n",
      "  Found checkpoints up to epoch 100\n",
      "  Processing epochs from 99 to 100\n",
      "  ‚è≠Ô∏è  Skipping epoch 100 (logits already exist)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Inference completed!\n",
      "   Processed: 16 experiments\n",
      "   Skipped: 0 experiments\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Main Inference Loop\n",
    "# ============================================================================\n",
    "# Model cache to avoid reloading models with the same architecture\n",
    "model_cache = {}\n",
    "\n",
    "def get_cached_model(arch):\n",
    "    \"\"\"Get or create a cached model for the given architecture.\"\"\"\n",
    "    if arch not in model_cache:\n",
    "        print(f\"Loading model with architecture: {arch}\")\n",
    "        model_cache[arch] = load_model(arch, nclass, mnist)\n",
    "    return model_cache[arch]\n",
    "\n",
    "# Process each experiment directory\n",
    "experiment_dirs = sorted([d for d in os.listdir(base_logdir) \n",
    "                          if os.path.isdir(os.path.join(base_logdir, d))])\n",
    "\n",
    "print(f\"Found {len(experiment_dirs)} directories in {base_logdir}\")\n",
    "print(f\"Filtering with regex: {regex_pattern}\")\n",
    "\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "\n",
    "for exp_dir in experiment_dirs:\n",
    "    # Check if directory matches regex\n",
    "    if re.search(regex_pattern, exp_dir) is None:\n",
    "        print(f\"Skipping '{exp_dir}' (doesn't match regex)\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    exp_path = os.path.join(base_logdir, exp_dir)\n",
    "    hparams_path = os.path.join(exp_path, \"hparams.json\")\n",
    "    \n",
    "    if not os.path.exists(hparams_path):\n",
    "        print(f\"Skipping '{exp_dir}' (no hparams.json found)\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Load hyperparameters to get architecture\n",
    "    with open(hparams_path, 'r') as f:\n",
    "        hparams = json.load(f)\n",
    "    arch = hparams['arch']\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Processing experiment: {exp_dir}\")\n",
    "    print(f\"Architecture: {arch}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Get or create model\n",
    "    model = get_cached_model(arch)\n",
    "    \n",
    "    # Set up checkpoint directory\n",
    "    checkpoint = objax.io.Checkpoint(exp_path, keep_ckpts=10, makedir=True)\n",
    "    max_epoch, last_ckpt = checkpoint.restore(model.vars())\n",
    "    \n",
    "    if max_epoch == 0:\n",
    "        print(f\"  ‚ö†Ô∏è  No checkpoints found for {exp_dir}, skipping\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    print(f\"  Found checkpoints up to epoch {max_epoch}\")\n",
    "    \n",
    "    # Create logits directory if it doesn't exist\n",
    "    logits_dir = os.path.join(exp_path, \"logits\")\n",
    "    os.makedirs(logits_dir, exist_ok=True)\n",
    "    \n",
    "    # Determine starting epoch\n",
    "    if from_epoch is not None:\n",
    "        first_epoch = from_epoch\n",
    "    else:\n",
    "        first_epoch = max_epoch - 1\n",
    "    \n",
    "    print(f\"  Processing epochs from {first_epoch} to {max_epoch}\")\n",
    "    \n",
    "    # Process each epoch\n",
    "    for epoch in range(first_epoch, max_epoch + 1):\n",
    "        ckpt_path = os.path.join(exp_path, \"ckpt\", f\"{epoch:010d}.npz\")\n",
    "        logits_path = os.path.join(logits_dir, f\"{epoch:010d}.npy\")\n",
    "        \n",
    "        # Skip if checkpoint doesn't exist\n",
    "        if not os.path.exists(ckpt_path):\n",
    "            continue\n",
    "        \n",
    "        # Skip if logits already generated\n",
    "        if os.path.exists(logits_path):\n",
    "            print(f\"  ‚è≠Ô∏è  Skipping epoch {epoch} (logits already exist)\")\n",
    "            continue\n",
    "        \n",
    "        # Load checkpoint\n",
    "        try:\n",
    "            start_epoch, last_ckpt = checkpoint.restore(model.vars(), epoch)\n",
    "            print(f\"  üìä Generating logits for epoch {epoch}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Failed to load checkpoint for epoch {epoch}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Generate logits in batches\n",
    "        stats = []\n",
    "        for i in range(0, len(xs_all), batch_size):\n",
    "            batch_x = xs_all[i:i+batch_size]\n",
    "            batch_y = ys_all[i:i+batch_size]\n",
    "            batch_logits = features(model, batch_x, batch_y)\n",
    "            stats.extend(batch_logits)\n",
    "        \n",
    "        # Save logits\n",
    "        # Shape will be (N, 1, num_augs, nclass) to match original format\n",
    "        logits_array = np.array(stats)[:, None, :, :]\n",
    "        np.save(logits_path, logits_array)\n",
    "        print(f\"  ‚úÖ Saved logits for epoch {epoch} (shape: {logits_array.shape})\")\n",
    "    \n",
    "    processed_count += 1\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚úÖ Inference completed!\")\n",
    "print(f\"   Processed: {processed_count} experiments\")\n",
    "print(f\"   Skipped: {skipped_count} experiments\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d38ef345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Computing scores from logits\n",
      "Found 16 experiment directories\n",
      "================================================================================\n",
      "Processing: experiment-0_16\n",
      "(50000, 1, 2)\n",
      "mean acc 0.95478\n",
      "Processing: experiment-10_16\n",
      "(50000, 1, 2)\n",
      "mean acc 0.9532\n",
      "Processing: experiment-11_16\n",
      "(50000, 1, 2)\n",
      "mean acc 0.953\n",
      "Processing: experiment-12_16\n",
      "(50000, 1, 2)\n",
      "mean acc 0.95402\n",
      "Processing: experiment-13_16\n",
      "(50000, 1, 2)\n",
      "mean acc 0.95152\n",
      "Processing: experiment-14_16\n",
      "(50000, 1, 2)\n",
      "mean acc 0.95402\n",
      "Processing: experiment-15_16\n",
      "(50000, 1, 2)\n",
      "mean acc 0.95206\n",
      "Processing: experiment-1_16\n",
      "(50000, 1, 2)\n",
      "mean acc 0.95634\n",
      "Processing: experiment-2_16\n",
      "(50000, 1, 2)\n",
      "mean acc 0.95366\n",
      "Processing: experiment-3_16\n",
      "(50000, 1, 2)\n",
      "mean acc 0.95286\n",
      "Processing: experiment-4_16\n",
      "(50000, 1, 2)\n",
      "mean acc 0.95384\n",
      "Processing: experiment-5_16\n",
      "(50000, 1, 2)\n",
      "mean acc 0.95192\n",
      "Processing: experiment-6_16\n",
      "(50000, 1, 2)\n",
      "mean acc 0.95062\n",
      "Processing: experiment-7_16\n",
      "(50000, 1, 2)\n",
      "mean acc 0.95442\n",
      "Processing: experiment-8_16\n",
      "(50000, 1, 2)\n",
      "mean acc 0.95206\n",
      "Processing: experiment-9_16\n",
      "(50000, 1, 2)\n",
      "mean acc 0.95704\n",
      "================================================================================\n",
      "‚úÖ Score computation completed!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Compute Scores from Logits\n",
    "# ============================================================================\n",
    "# Use the functions from score.py directly\n",
    "from score import load_one\n",
    "\n",
    "# Set up the logdir and labels for score.py (it uses global variables)\n",
    "import score as score_module\n",
    "score_module.logdir = base_logdir\n",
    "score_module.labels = ys_all\n",
    "\n",
    "# Process all experiment directories matching regex\n",
    "experiment_dirs = sorted([d for d in os.listdir(base_logdir) \n",
    "                          if os.path.isdir(os.path.join(base_logdir, d))])\n",
    "matching_dirs = [d for d in experiment_dirs if re.search(regex_pattern, d)]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Computing scores from logits\")\n",
    "print(f\"Found {len(matching_dirs)} experiment directories\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for exp_dir in matching_dirs:\n",
    "    print(f\"Processing: {exp_dir}\")\n",
    "    load_one(exp_dir)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ Score computation completed!\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lra-env)",
   "language": "python",
   "name": "lra-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
