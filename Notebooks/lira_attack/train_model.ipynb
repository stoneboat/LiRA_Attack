{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72786dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Navigate to the parent directory of the project structure\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), '../../'))\n",
    "src_dir = os.path.join(project_dir, 'src')\n",
    "data_dir = os.path.join(project_dir, 'data')\n",
    "fig_dir = os.path.join(project_dir, 'fig')\n",
    "logs_dir = os.path.join(project_dir, 'logs')\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "# Add the src directory to sys.path\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71c091bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 18:20:31.959953: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-05 18:20:32.016387: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-05 18:20:32.815058: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/tmp/python-venv/lra_venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting run 3 that did not complete.\n",
      "Creating experiment directory: /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/lira_attack/logs/exp/cifar10/experiment-3_16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from objax.util import EasyDict\n",
    "from absl import flags\n",
    "\n",
    "# Define flags that train.py uses (normally defined in if __name__ == '__main__')\n",
    "flags.DEFINE_string('arch', 'cnn32-3-mean', 'Model architecture.')\n",
    "flags.DEFINE_float('lr', 0.1, 'Learning rate.')\n",
    "flags.DEFINE_string('dataset', 'cifar10', 'Dataset.')\n",
    "flags.DEFINE_float('weight_decay', 0.0005, 'Weight decay ratio.')\n",
    "flags.DEFINE_integer('batch', 256, 'Batch size')\n",
    "flags.DEFINE_integer('epochs', 501, 'Training duration in number of epochs.')\n",
    "flags.DEFINE_string('logdir', 'experiments', 'Directory where to save checkpoints and tensorboard data.')\n",
    "flags.DEFINE_integer('seed', None, 'Training seed.')\n",
    "flags.DEFINE_float('pkeep', .5, 'Probability to keep examples.')\n",
    "flags.DEFINE_integer('expid', None, 'Experiment ID')\n",
    "flags.DEFINE_integer('num_experiments', None, 'Number of experiments')\n",
    "flags.DEFINE_string('augment', 'weak', 'Strong or weak augmentation')\n",
    "flags.DEFINE_integer('only_subset', None, 'Only train on a subset of images.')\n",
    "flags.DEFINE_integer('dataset_size', 50000, 'number of examples to keep.')\n",
    "flags.DEFINE_integer('eval_steps', 1, 'how often to get eval accuracy.')\n",
    "flags.DEFINE_integer('abort_after_epoch', None, 'stop trainin early at an epoch')\n",
    "flags.DEFINE_integer('save_steps', 10, 'how often to get save model.')\n",
    "flags.DEFINE_integer('patience', None, 'Early stopping after this many epochs without progress')\n",
    "flags.DEFINE_bool('tunename', False, 'Use tune name?')\n",
    "\n",
    "\n",
    "from train import get_data, network, MemModule\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Parse flags (required before accessing FLAGS values)\n",
    "# Mark flags as parsed so we can access them without command-line args\n",
    "FLAGS.mark_as_parsed()\n",
    "\n",
    "# ============================================================================\n",
    "# Training Parameters - Set these directly\n",
    "# ============================================================================\n",
    "# Dataset and architecture\n",
    "dataset = 'cifar10'\n",
    "arch = 'wrn28-2'\n",
    "\n",
    "# Training configuration\n",
    "epochs = 100\n",
    "save_steps = 20\n",
    "batch = 256\n",
    "lr = 0.1\n",
    "weight_decay = 0.0005\n",
    "augment = 'weak'\n",
    "pkeep = 0.5\n",
    "\n",
    "# Experiment configuration\n",
    "expid = 3\n",
    "num_experiments = 16\n",
    "seed = None  # Will be auto-generated if None\n",
    "\n",
    "# Optional parameters\n",
    "only_subset = None\n",
    "patience = None\n",
    "dataset_size = 50000\n",
    "eval_steps = 1\n",
    "tunename = False\n",
    "\n",
    "# ============================================================================\n",
    "# Training Logic (from train.py main function)\n",
    "# ============================================================================\n",
    "\n",
    "# Disable GPU for TensorFlow (JAX will handle GPU)\n",
    "tf.config.experimental.set_visible_devices([], \"GPU\")\n",
    "\n",
    "# Set seed\n",
    "if seed is None:\n",
    "    seed = np.random.randint(0, 1000000000)\n",
    "    seed ^= int(time.time())\n",
    "\n",
    "# Create args dictionary\n",
    "args = EasyDict(\n",
    "    arch=arch,\n",
    "    lr=lr,\n",
    "    batch=batch,\n",
    "    weight_decay=weight_decay,\n",
    "    augment=augment,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "assert expid is not None and num_experiments is not None\n",
    "\n",
    "base_logdir = os.path.join(logs_dir, 'exp', 'cifar10')\n",
    "os.makedirs(base_logdir, exist_ok=True)\n",
    "\n",
    "logdir_path = f\"experiment-{expid}_{num_experiments}\"\n",
    "logdir_path = os.path.join(base_logdir, logdir_path)\n",
    "\n",
    "if os.path.exists(os.path.join(logdir_path, \"ckpt\", f\"{epochs:010d}.npz\")):\n",
    "    print(f\"Run {expid} already completed.\")\n",
    "else:\n",
    "    if os.path.exists(logdir_path):\n",
    "        print(f\"Deleting run {expid} that did not complete.\")\n",
    "        shutil.rmtree(logdir_path)\n",
    "    \n",
    "    print(f\"Creating experiment directory: {logdir_path}\")\n",
    "    os.makedirs(logdir_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf02737c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "No checkpoints found. Skipping restoring variables.\n",
      "Epoch 0001  Loss 1.84  Accuracy 20.87\n",
      "Epoch 0002  Loss 1.35  Accuracy 27.72\n",
      "Epoch 0003  Loss 1.07  Accuracy 41.02\n",
      "Epoch 0004  Loss 0.94  Accuracy 51.19\n",
      "Epoch 0005  Loss 0.83  Accuracy 58.73\n",
      "Epoch 0006  Loss 0.74  Accuracy 63.18\n",
      "Epoch 0007  Loss 0.68  Accuracy 66.55\n",
      "Epoch 0008  Loss 0.63  Accuracy 69.64\n",
      "Epoch 0009  Loss 0.59  Accuracy 71.63\n",
      "Epoch 0010  Loss 0.56  Accuracy 73.72\n",
      "Epoch 0011  Loss 0.51  Accuracy 75.35\n",
      "Epoch 0012  Loss 0.49  Accuracy 76.94\n",
      "Epoch 0013  Loss 0.48  Accuracy 78.18\n",
      "Epoch 0014  Loss 0.44  Accuracy 79.42\n",
      "Epoch 0015  Loss 0.45  Accuracy 80.21\n",
      "Epoch 0016  Loss 0.42  Accuracy 81.26\n",
      "Epoch 0017  Loss 0.41  Accuracy 82.10\n",
      "Epoch 0018  Loss 0.39  Accuracy 82.94\n",
      "Epoch 0019  Loss 0.38  Accuracy 83.74\n",
      "Epoch 0020  Loss 0.37  Accuracy 84.37\n",
      "Epoch 0021  Loss 0.35  Accuracy 84.89\n",
      "Epoch 0022  Loss 0.34  Accuracy 85.43\n",
      "Epoch 0023  Loss 0.34  Accuracy 85.86\n",
      "Epoch 0024  Loss 0.33  Accuracy 86.41\n",
      "Epoch 0025  Loss 0.31  Accuracy 86.70\n",
      "Epoch 0026  Loss 0.30  Accuracy 87.13\n",
      "Epoch 0027  Loss 0.31  Accuracy 87.41\n",
      "Epoch 0028  Loss 0.30  Accuracy 87.76\n",
      "Epoch 0029  Loss 0.28  Accuracy 87.99\n",
      "Epoch 0030  Loss 0.28  Accuracy 88.21\n",
      "Epoch 0031  Loss 0.27  Accuracy 88.39\n",
      "Epoch 0032  Loss 0.27  Accuracy 88.63\n",
      "Epoch 0033  Loss 0.25  Accuracy 88.96\n",
      "Epoch 0034  Loss 0.26  Accuracy 89.05\n",
      "Epoch 0035  Loss 0.25  Accuracy 89.27\n",
      "Epoch 0036  Loss 0.25  Accuracy 89.33\n",
      "Epoch 0037  Loss 0.24  Accuracy 89.56\n",
      "Epoch 0038  Loss 0.23  Accuracy 89.71\n",
      "Epoch 0039  Loss 0.23  Accuracy 89.83\n",
      "Epoch 0040  Loss 0.23  Accuracy 89.89\n",
      "Epoch 0041  Loss 0.23  Accuracy 90.02\n",
      "Epoch 0042  Loss 0.22  Accuracy 90.17\n",
      "Epoch 0043  Loss 0.22  Accuracy 90.35\n",
      "Epoch 0044  Loss 0.20  Accuracy 90.37\n",
      "Epoch 0045  Loss 0.20  Accuracy 90.48\n",
      "Epoch 0046  Loss 0.20  Accuracy 90.48\n",
      "Epoch 0047  Loss 0.19  Accuracy 90.55\n",
      "Epoch 0048  Loss 0.19  Accuracy 90.64\n",
      "Epoch 0049  Loss 0.18  Accuracy 90.68\n",
      "Epoch 0050  Loss 0.19  Accuracy 90.68\n",
      "Epoch 0051  Loss 0.18  Accuracy 90.72\n",
      "Epoch 0052  Loss 0.18  Accuracy 90.82\n",
      "Epoch 0053  Loss 0.17  Accuracy 90.97\n",
      "Epoch 0054  Loss 0.17  Accuracy 90.99\n",
      "Epoch 0055  Loss 0.17  Accuracy 90.98\n",
      "Epoch 0056  Loss 0.17  Accuracy 91.06\n",
      "Epoch 0057  Loss 0.16  Accuracy 91.11\n",
      "Epoch 0058  Loss 0.16  Accuracy 91.06\n",
      "Epoch 0059  Loss 0.15  Accuracy 91.14\n",
      "Epoch 0060  Loss 0.15  Accuracy 91.18\n",
      "Epoch 0061  Loss 0.15  Accuracy 91.26\n",
      "Epoch 0062  Loss 0.15  Accuracy 91.32\n",
      "Epoch 0063  Loss 0.13  Accuracy 91.30\n",
      "Epoch 0064  Loss 0.13  Accuracy 91.25\n",
      "Epoch 0065  Loss 0.13  Accuracy 91.22\n",
      "Epoch 0066  Loss 0.13  Accuracy 91.26\n",
      "Epoch 0067  Loss 0.12  Accuracy 91.26\n",
      "Epoch 0068  Loss 0.12  Accuracy 91.32\n",
      "Epoch 0069  Loss 0.12  Accuracy 91.44\n",
      "Epoch 0070  Loss 0.10  Accuracy 91.48\n",
      "Epoch 0071  Loss 0.12  Accuracy 91.52\n",
      "Epoch 0072  Loss 0.10  Accuracy 91.61\n",
      "Epoch 0073  Loss 0.11  Accuracy 91.57\n",
      "Epoch 0074  Loss 0.10  Accuracy 91.60\n",
      "Epoch 0075  Loss 0.09  Accuracy 91.70\n",
      "Epoch 0076  Loss 0.10  Accuracy 91.65\n",
      "Epoch 0077  Loss 0.10  Accuracy 91.71\n",
      "Epoch 0078  Loss 0.09  Accuracy 91.75\n",
      "Epoch 0079  Loss 0.08  Accuracy 91.81\n",
      "Epoch 0080  Loss 0.09  Accuracy 91.77\n",
      "Epoch 0081  Loss 0.08  Accuracy 91.82\n",
      "Epoch 0082  Loss 0.08  Accuracy 91.86\n",
      "Epoch 0083  Loss 0.07  Accuracy 91.90\n",
      "Epoch 0084  Loss 0.06  Accuracy 92.00\n",
      "Epoch 0085  Loss 0.07  Accuracy 92.00\n",
      "Epoch 0086  Loss 0.05  Accuracy 92.01\n",
      "Epoch 0087  Loss 0.06  Accuracy 91.96\n",
      "Epoch 0088  Loss 0.06  Accuracy 91.97\n",
      "Epoch 0089  Loss 0.06  Accuracy 92.07\n",
      "Epoch 0090  Loss 0.04  Accuracy 92.15\n",
      "Epoch 0091  Loss 0.04  Accuracy 92.06\n",
      "Epoch 0092  Loss 0.04  Accuracy 91.99\n",
      "Epoch 0093  Loss 0.04  Accuracy 92.07\n",
      "Epoch 0094  Loss 0.03  Accuracy 92.12\n",
      "Epoch 0095  Loss 0.03  Accuracy 92.20\n",
      "Epoch 0096  Loss 0.02  Accuracy 92.25\n",
      "Epoch 0097  Loss 0.02  Accuracy 92.09\n",
      "Epoch 0098  Loss 0.02  Accuracy 92.13\n",
      "Epoch 0099  Loss 0.02  Accuracy 92.06\n",
      "Epoch 0100  Loss 0.01  Accuracy 92.05\n",
      "--------------------------------------------------------------------------------\n",
      "✅ Training completed! Results saved to /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/lira_attack/logs/exp/cifar10/experiment-3_16\n"
     ]
    }
   ],
   "source": [
    "# Create configuration dictionary for get_data\n",
    "config = {\n",
    "    'logdir': logs_dir,\n",
    "    'dataset': dataset,\n",
    "    'dataset_size': dataset_size,\n",
    "    'num_experiments': num_experiments,\n",
    "    'expid': expid,\n",
    "    'pkeep': pkeep,\n",
    "    'only_subset': only_subset,\n",
    "    'augment': augment,\n",
    "    'batch': batch,\n",
    "    'data_dir': data_dir\n",
    "}\n",
    "\n",
    "# Get data - pass config dictionary\n",
    "train_data, test_data, xs, ys, keep, nclass = get_data(seed, config)\n",
    "\n",
    "# Define the network and training module\n",
    "tm = MemModule(\n",
    "    network(arch), \n",
    "    nclass=nclass,\n",
    "    mnist=(dataset == 'mnist'),\n",
    "    epochs=epochs,\n",
    "    expid=expid,\n",
    "    num_experiments=num_experiments,\n",
    "    pkeep=pkeep,\n",
    "    save_steps=save_steps,\n",
    "    only_subset=only_subset,\n",
    "    **args\n",
    ")\n",
    "\n",
    "# Save hyperparameters\n",
    "params = {}\n",
    "params.update(tm.params)\n",
    "\n",
    "with open(os.path.join(logdir_path, 'hparams.json'), 'w') as f:\n",
    "    json.dump(params, f)\n",
    "np.save(os.path.join(logdir_path, 'keep.npy'), keep)\n",
    "\n",
    "# Train\n",
    "print(\"-\" * 80)\n",
    "tm.train(epochs, len(xs), train_data, test_data, logdir_path,\n",
    "            save_steps=save_steps, patience=patience, eval_steps=eval_steps)\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"✅ Training completed! Results saved to {logdir_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lra-env)",
   "language": "python",
   "name": "lra-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
